{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "breathing-chest",
   "metadata": {},
   "source": [
    "# Modelo de clasificación por formato"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-traveler",
   "metadata": {},
   "source": [
    "#### EN ESTE NOTEBOOK VAMOS A COMPROBAR SI PUDIERAMOS ESTABLECER ALGUNA CONEXION ENTRE LOS DATOS QUE PODEMOS OBTENER DEL FORMATO Y NUESTRO TARGET DE ANALISIS DE SENTIMIENTO, DE FORMA BINARIA.\n",
    "\n",
    "![](images/text.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-arbitration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importo las librerias que voy a utilizar en este notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-intervention",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../elmundo/input/df_elmundo_fin.csv',index_col=[0], parse_dates=True)\n",
    "df.sample(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-neighbor",
   "metadata": {},
   "source": [
    "## Posibles features\n",
    "\n",
    "     - dia de la semana del 1 al 7\n",
    "     - mes\n",
    "     - año\n",
    "     - numero de adjetivos, verbos, sustantivos)\n",
    "     - numero de palabras relativas al maximo\n",
    "     - numero de stopwords\n",
    "     - numero de palabras unicas\n",
    "     - autores = LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fecha'] = pd.to_datetime(df.Fecha)\n",
    "numeric_df = df[['Fecha', 'Texto','Autores','Coincidencia_elmundo_relativa','Scores_zero',\n",
    "                 'Analisis_sentimiento_scd']]\n",
    "numeric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dia de la semana, dia y mes en numero.\n",
    "numeric_df['dia'] = numeric_df.Fecha.dt.day\n",
    "numeric_df['dia_semana'] = numeric_df.Fecha.dt.dayofweek\n",
    "numeric_df['mes'] = numeric_df.Fecha.dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "# número de stopwords.\n",
    "stop_spa = stopwords.words('spanish')\n",
    "\n",
    "def counter_stopwords(fila):\n",
    "    counter = 0\n",
    "    for i in fila.split():\n",
    "        if i in stop_spa:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "numeric_df['num_stop_words'] = numeric_df['Texto'].apply(lambda x: counter_stopwords(x))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numero de palabras\n",
    "def counter_words(fila):\n",
    "    counter = 0\n",
    "    for i in fila.split():\n",
    "           counter += 1\n",
    "    return counter\n",
    "numeric_df['num_palabras'] = numeric_df.Texto.apply(lambda x: counter_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df['len_total'] = numeric_df.Texto.apply(lambda x: len(x))\n",
    "numeric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-thought",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero de palabras únicas.\n",
    "def counter_uniques(fila):\n",
    "    for i in fila:\n",
    "        \n",
    "        return len(set(i))\n",
    "    \n",
    "numeric_df['unique'] = numeric_df['Texto'].apply(lambda x: counter_uniques(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contamos los signos de excl, interrogacion y puntuaciones\n",
    "numeric_df['num_exclamacion'] = numeric_df['Texto'].apply(lambda x: x.count('!'))\n",
    "numeric_df['num_interrogaciones'] = numeric_df['Texto'].apply(lambda x: x.count('?'))\n",
    "numeric_df['num_puntuaciones'] = numeric_df['Texto'].apply(lambda x: sum(x.count(i) for i in '.,;:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos X e Y\n",
    "cols = ['num_stop_words', 'num_palabras', 'len_total',\n",
    "       'unique','num_exclamacion', 'num_interrogaciones',\n",
    "       'num_puntuaciones']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-reform",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el target y normalizamos los datos.\n",
    "X = numeric_df[cols]\n",
    "y = np.where(numeric_df['Analisis_sentimiento_scd'] > 0,1,0)\n",
    "\n",
    "X['num_stop_words'] = X.num_stop_words/X.num_stop_words.max()\n",
    "X['len_total'] = X.len_total/X.len_total.max()\n",
    "X['unique'] = X['unique']/X['unique'].max()\n",
    "X['num_puntuaciones'] = X.num_puntuaciones/X.num_puntuaciones.max()\n",
    "X['num_palabras'] = X.num_palabras/X.num_palabras.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos en train y test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                   y,\n",
    "                                                   test_size=0.15,\n",
    "                                                   random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el modelo con keras\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(input_dim = 7,\n",
    "                      units=3,\n",
    "                      activation='sigmoid'),\n",
    "    keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "opt = keras.optimizers.Adam(learning_rate=0.002)\n",
    "model.compile(loss = 'binary_crossentropy', \n",
    "             optimizer=opt, metrics='accuracy')\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience = 4,\n",
    "                                                 restore_best_weights=True)\n",
    "history = model.fit(X_train,\n",
    "                   y_train,\n",
    "                   epochs=50,\n",
    "                   batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos comprobar que le cuesta mucho encontrar patrones en los datos, y el accuracy siempre se mantiene\n",
    "# en el porcentaje de balanceo del target.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-lafayette",
   "metadata": {},
   "source": [
    "Vamos a probar con otros modelos por si pudieran encontrar algo diferente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-duncan",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression()\n",
    "params = {\n",
    "    'C': [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "    'penalty': ['l2','l1'],\n",
    "    'solver':['liblinear'],\n",
    "    'max_iter':[50,100,200]\n",
    "}\n",
    "grid = GridSearchCV(estimator=lg,param_grid=params, cv=4)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-depression",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lg = grid.best_estimator_.predict(X_test)\n",
    "print(classification_report(y_pred_lg,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-dialogue",
   "metadata": {},
   "source": [
    "## AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier()\n",
    "params = {\n",
    "    'n_estimators' : [10,20,30,40,50,60,70,80,200],\n",
    "    'learning_rate' : [0.1,0.3,0.4,0.7,1,1.5],\n",
    "    'algorithm' : ['SAMME','SAMME.R']\n",
    "}\n",
    "grid2 = GridSearchCV(estimator=ada,\n",
    "                    param_grid = params)\n",
    "grid2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-seating",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ada = grid2.best_estimator_.predict(X_test)\n",
    "print(classification_report(y_pred_ada,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-broadcast",
   "metadata": {},
   "source": [
    "**Como podemos ver, todos los modelos nos dan los mismos resultados con estos datos;\n",
    "Luego lo que da que pensar que el problema viene del etiquetado o de la extraccion de los mismos**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
