{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "potential-greenhouse",
   "metadata": {},
   "source": [
    "## Análisis exploratorio de los datos\n",
    "![EDA](images/eda.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importo las librerias que voy a utilizar en este notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import nltk\n",
    "\n",
    "from unicodedata import normalize\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForSeqLM, AutoTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-gasoline",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../elmundo/input/df_elmundo.csv', encoding='latin1',\n",
    "                 index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-artwork",
   "metadata": {},
   "source": [
    "#### Nota: \n",
    "*Al final vamos a tratar todo el texto por igual, duplicando 'otros datos', tanto en titular como en noticia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-property",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rehacemos el dataset, y lo sobreescribimos, ya que lo tenemos en el csv.\n",
    "df1 = df.iloc[:,1:]\n",
    "df2 = df.loc[:,['Titulares','Otros_datos']]\n",
    "df2.columns = ['Texto','Otros_datos']\n",
    "df = pd.concat([df1,df2], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-element",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tenemos la columna de Texto, que es la que necesitamos, y otros datos que hemos scrappeado en las noticias.\n",
    "musica_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para poder guardar mas comodamente el csv y que no de problemas posteriormente al tokenizar, es pasar a minúscula,\n",
    "# y quitar las tildes.\n",
    "\n",
    "musica_df['Texto'] = musica_df.Texto.apply(lambda x: str.lower(x))\n",
    "\n",
    "a,b = 'áéíóúü','aeiouu'\n",
    "trans = str.maketrans(a,b)\n",
    "musica_df['Texto'] = musica_df.Texto.apply(lambda x: x.translate(trans))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se puede observar que en la columna Otros_datos hay diferentes datos separados por ('\\n').\n",
    "musica_df['Otros_datos'] = musica_df.Otros_datos.apply(lambda x: x.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos devuelve una lista con varios elementos.\n",
    "musica_df['Clase'] = musica_df.Otros_datos.apply(lambda x: x[0])\n",
    "musica_df['Coincidencia'] = musica_df.Otros_datos.apply(lambda x: x[2])\n",
    "musica_df['varios'] = musica_df.Otros_datos.apply(lambda x: x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-turkish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos que en el ultimo elemento, podemos sacar la fecha:\n",
    "# La forma mas fácil que se me ha ocurrido ha sido con una expresión regular.\n",
    "\n",
    "fechas = []\n",
    "for i in musica_df['varios']:\n",
    "    fechas.append(''.join(re.findall(r'[0-9/]', i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "musica_df['Fecha'] = fechas\n",
    "musica_df['Fecha'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para arreglar las fechas, bastaría con quedarnos con los primeros 10 caracteres de la cadena\n",
    "fechas_bueno = []\n",
    "for i in musica_df.Fecha:\n",
    "    fechas_bueno.append(i[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "musica_df['Fecha'] = fechas_bueno\n",
    "musica_df['Fecha'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo pasamos a formato datetime\n",
    "\n",
    "musica_df['Fecha'] = pd.to_datetime(musica_df.Fecha)\n",
    "musica_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una columna con los autores, que también estaban en el tercer elemento de la lista, luego aplico la misma\n",
    "# solución que antes.\n",
    "autores = []\n",
    "for i in musica_df['varios']:\n",
    "    autores.append(''.join(re.findall(r'[\\sA-Z|]', i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "musica_df['Autores'] = autores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiamos la columna coincidencia\n",
    "coincidencias_num = []\n",
    "for i in musica_df['Coincidencia']:\n",
    "    coincidencias_num.append((''.join(re.findall(r'[0-9]', i))))\n",
    "\n",
    "musica_df['Coincidencia_elmundo'] = coincidencias_num\n",
    "musica_df['Coincidencia_elmundo'] = musica_df['Coincidencia_elmundo'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-wednesday",
   "metadata": {},
   "source": [
    "**La columna coincidencia va de 0 a 100, asi que directamente he decido sacar la coincidencia relativa sobre el máximo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "musica_df['Coincidencia_elmundo_relativa'] = musica_df['Coincidencia_elmundo']/musica_df['Coincidencia_elmundo'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y ya nos deshacemos las columnas que habiamos scrapeado.\n",
    "musica_df_clean = musica_df.drop(columns=['Otros_datos','varios', 'Coincidencia'])\n",
    "musica_df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-ethiopia",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-retailer",
   "metadata": {},
   "source": [
    "*Según Wikipedia, cuna de la sabiduría popular:*\n",
    "\n",
    "    - Bidirectional Encoder Representations from Transformers\n",
    "    - BERT fue creado y publicado en 2018 por Jacob Devlin y sus colegas de Google.\n",
    "    + Es una técnica basada en redes neuronales para el pre-entrenamiento del procesamiento del lenguaje natural (PLN) desarrollada por Google.\n",
    "\n",
    "![](images/bert.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-fever",
   "metadata": {},
   "source": [
    "Lo que más me ha llamado la atención, que de igual manera, así lo expresa Wikipedia;\n",
    "Es la capacidad de interpretar el sentido de la oración a través de los algoritmos de redes neuronales, y no se base en la interpretación básica de cada elemento en función de x características.\n",
    "\n",
    "**Por este motivo, he visto fundamental utilizar transformers basados en BERT, en particular, el modelo de traducción, que es el modelo Helsinki-NLP para la traducción de inglés a español**\n",
    "\n",
    "\n",
    "He comprobado que existen mas y mejores funcionalidades para NLP en inglés que en español, por lo que vamos a traducir el texto y trabajar con él, en inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-arbitration",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-es-en')\n",
    "tokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-es-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = pipeline('translation_es_to_en', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos una prueba\n",
    "text = 'Mi madre me hace la comida los domingos y yo no voy mucho a verla'\n",
    "translated_text = translation(text, max_length=40)[0]['translation_text']\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a crear una lista con los 1000 textos traducidos para posteriormente incorporarlos a dataframe\n",
    "# Nota: lo he hecho un for y un contador, ya que tarda bastante tiempo. En mi este caso 27min.\n",
    "%%time\n",
    "english_texts = []\n",
    "\n",
    "\n",
    "for j,i in enumerate(musica_df_clean['Texto']):\n",
    "    english_texts.append(translation(i, max_length=100)[0]['translation_text'])\n",
    "    if j % 10 == 0:\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "musica_df_clean['English_text'] = english_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reordeno las columnas\n",
    "cols = ['Fecha','Texto','English_text','Autores','Coincidencia_elmundo','Coincidencia_elmundo_relativa','Clase']\n",
    "musica_df_clean = musica_df_clean[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solamente hay dos clases Noticia == 1 o Foto == 0\n",
    "\n",
    "musica_df_clean['Label_clase'] = np.where(musica_df_clean['Clase'] == 'noticia',1,0)\n",
    "\n",
    "# Estas clases estan extremadamente desbalanceadas, y tampoco parace que vaya a aportarnos mucho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos el score que le da huggingface con las tematicas mas buscadas en 2020 en españa y la nuestra.(inglés)\n",
    "# Buscador de Google.\n",
    "candidate_labels = ['music','coronavirus','elections']\n",
    "classifier_zero = pipeline('zero-shot-classification')\n",
    "text_prueba = 'i have been so sick this year'\n",
    "classifier_zero(text_prueba,\n",
    "               candidate_labels=candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "#De igual manera pongo un contador para ver como va avanzando\n",
    "scores = []\n",
    "labels = []\n",
    "for j,i in enumerate(df['English_text']):\n",
    "    dictionary = classifier_zero(i, candidate_labels=candidate_labels)\n",
    "    scores.append(dictionary['scores'][0])\n",
    "    labels.append(dictionary['labels'][0])\n",
    "    if j % 20 == 0:\n",
    "        print('Voy por el texto numero {}'.format(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Por ultimo el analisis de sentimiento, guardamos tambien las etiquetas para posteriormente multiplicar por -1\n",
    "#los sentimientos negativos\n",
    "classifier_sentiment = pipeline(\"sentiment-analysis\")\n",
    "scores_sentiment = []\n",
    "labels_sentiment = []\n",
    "for j,i in enumerate(df['English_text']):\n",
    "    dictionary = classifier_sentiment(i)\n",
    "    scores_sentiment.append(dictionary[0]['score'])\n",
    "    labels_sentiment.append(dictionary[0]['label'])\n",
    "    if j % 20 == 0:\n",
    "        print('Voy por el texto numero {}'.format(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-measurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metemos todas las listas al dataframe, el label de sentimiento lo transformamos a 1 si es positivo y -1 si es negativo\n",
    "# Posteriormente multiplicamos este label de sentimiento por el score en el analisis, y de esta manera lo centramos en 0\n",
    "\n",
    "musica_df_clean['Clasif_label_zero'] = labels\n",
    "musica_df_clean['Scores_zero'] = scores\n",
    "musica_df_clean['Analisis_sentimiento'] = scores_sentiment\n",
    "musica_df_clean['Label_sentimiento'] = labels_sentiment\n",
    "\n",
    "musica_df_clean['Label_sentimiento'] = np.where(df['Label_sentimiento'] == 'POSITIVE',1,-1)\n",
    "musica_df_clean['Analisis_sentimiento_scd'] = df['Analisis_sentimiento'] * df['Label_sentimiento']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo guardamos en csv, para no tener que volver a repetir todo el entrenamiento.\n",
    "musica_df_clean.to_csv('../elmundo/input/df_elmundo_fin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volvemos a seleccionar las features.\n",
    "df_english = pd.read_csv('../elmundo/input/musica_df_clean.csv',index_col=[0])\n",
    "columns = ['Fecha','English_text','Autores','Analisis_sentimiento_scd','Scores_zero','Clasif_label_zero']\n",
    "\n",
    "df_english = df[columns]\n",
    "df_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a ver cuantos registros tiene cada tematica\n",
    "# Vemos cuantos registros tenemos de cada tematica.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_palette('cubehelix')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "sns.histplot(x='Clasif_label_zero',data=df_english,ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sentimiento_clases = df_english.groupby('Clasif_label_zero')[['Analisis_sentimiento_scd']].mean().reset_index()\n",
    "top_sentimiento_clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentimiento positivo.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "sns.set_palette('inferno')\n",
    "sns.set_style('whitegrid')\n",
    "sns.barplot(x='Clasif_label_zero',y='Analisis_sentimiento_scd',data=top_sentimiento_clases, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTRIBUCION SENTIMIENTO\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "sns.histplot(x='Analisis_sentimiento_scd', kde=True, data=df_english, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-philadelphia",
   "metadata": {},
   "source": [
    "***Esta variable que hemos creado con Huggingface, identifica los sentimientos de las palabras de forma positiva o negativa dandoles un score a ambas clases ('Positivo y Negativo') sobre 1, el score que gane es el que se le adjudica a la palabra***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A través de esta función tokenizamos solo las palabras,\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "df_english.loc[:,'English_text_token'] = df_english['English_text'].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a ver si existe alguna relación entre las tematicas y el analisis de sentimiento, cogiendo de muestra:\n",
    "# los 20 autores con mas score de sentimiento positivo y los 20 autores con mas score de sentimiento negativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "autores_sentimiento = df_english.groupby('Autores')[['Analisis_sentimiento_scd']].mean().reset_index()\n",
    "\n",
    "top_autores_positivos = autores_sentimiento.nlargest(20,'Analisis_sentimiento_scd')\n",
    "\n",
    "top_autores_negativos = autores_sentimiento.nsmallest(20, 'Analisis_sentimiento_scd')\n",
    "\n",
    "autores_top = pd.concat([top_autores_positivos,top_autores_negativos])\n",
    "tema_pred = []\n",
    "for i in autores_top['Autores']:\n",
    "    tema_pred.append(df_english.loc[df_english['Autores']==i,'Clasif_label_zero'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "autores_top['tema'] = tema_pred\n",
    "sns.set_palette('tab20')\n",
    "fig, ax = plt.subplots(figsize=(20,7))\n",
    "sns.scatterplot(x='Autores',y='Analisis_sentimiento_scd',hue='tema',data=autores_top, ax=ax)\n",
    "plt.xticks(rotation=45);\n",
    "\n",
    "# Se pueden observar que a pesar de que las categorias estan balanceadas la tematica \"coronavirus\", abarca las \n",
    "# las tematicas principales de los autores valorados con mas sentimiento positivo y también las que mas sentimiento\n",
    "# negativo se le atribuye."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-polyester",
   "metadata": {},
   "source": [
    "##### Comentario\n",
    "En este grafico no hemos podido observar gran relación basandonos en los comentarios top valoracion sentimiento y la tematica a la que pertenecen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos si existe alguna relación del analisis de sentimiento dependiendo de la fecha\n",
    "df_date_sentimiento = df[['Fecha','Analisis_sentimiento_scd']].set_index('Fecha')\n",
    "df_date_sentimiento.index = pd.to_datetime(df_date_sentimiento.index)\n",
    "monthly = df_date_sentimiento.resample('M').sum()\n",
    "monthly.plot(style=[':'])\n",
    "plt.title('Analisis sentimiento en los articulos por meses');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly = df_date_sentimiento.resample('W').sum()\n",
    "weekly.plot(style=[':'])\n",
    "plt.title('Analisis sentimiento en los articulos por meses, semanalmente')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-england",
   "metadata": {},
   "source": [
    "##### Comentario\n",
    "\n",
    "Aquí podemos ver que la mayoria de artículos con sentimiento positivo se concentraron en los meses de verano con ciertas caídas puntuales.\n",
    "Por lo general se pueden apreciar unas medias de sentimiento muy bajas durante el resto del año."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No puede ser menor de 0.33\n",
    "df_labels = df_english.groupby('Clasif_label_zero')[['Scores_zero']].mean().reset_index()\n",
    "sns.distplot(df_labels['Scores_zero'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Clasif_label_zero',y='Scores_zero',data=df_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-cholesterol",
   "metadata": {},
   "source": [
    "Como era de esperar las clasificadas como musica tienen un mayor score, por lo que tienen un menor error.\n",
    "La busqueda de estos artículos viene dada por la misma palabra 'musica'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-binding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a crear un array de las palabras sin las stopwords. La llamo Clean_words\n",
    "\n",
    "def stop(raw):\n",
    "    return [word.lower() for word in raw if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "df_english['Clean_words'] = df_english['English_text_token'].apply(lambda x: stop(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-construction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aun que NLTK tiene una funcion propia, es facil hacerlo manualmente.\n",
    "# Creamos un diccionario con la frecuencia de cada palabra.\n",
    "dictionary = dict()\n",
    "\n",
    "for i in df_english['Clean_words']:\n",
    "    for j in i:\n",
    "        if j in dictionary:\n",
    "            dictionary[j] +=1\n",
    "        else:\n",
    "            dictionary[j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "palabras = []\n",
    "veces = []\n",
    "for (key,value) in dictionary.items():\n",
    "    if value > 15:\n",
    "        palabras.append(key)\n",
    "        veces.append(value)\n",
    "        \n",
    "top_freq = pd.DataFrame({'palabras': palabras,\n",
    "                        'veces':veces})      \n",
    "top15_freq = top_freq.nlargest(15, 'veces')\n",
    "\n",
    "sns.swarmplot(x='palabras', y='veces', data=top15_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Despues de quitar las stopwords y los signos de puntuación hay {} palabras diferentes'.format(len(dictionary))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una función para pintar un wordcloud con las palabras que tienen mas freq.\n",
    "def cloud(text):\n",
    "    wordcloud = WordCloud(background_color='black').generate(' '.join([i for i in text.str.upper()]))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.title('Article Words')\n",
    "\n",
    "cloud(df_english['English_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-steps",
   "metadata": {},
   "source": [
    "## Stemming Words\n",
    "Intentamos buscar los lexemas, raices comunes de las palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para esto vamos a utilizar PorterStemmer de nltk.\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def pstem(reg):\n",
    "    return [stemmer.stem(word) for word in reg]\n",
    "    \n",
    "\n",
    "df_english['stems'] = df_english['Clean_words'].apply(lambda x: pstem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-times",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_stems = dict()\n",
    "\n",
    "for i in df_english['stems']:\n",
    "    for j in i:\n",
    "        if j in dictionary_stems:\n",
    "            dictionary_stems[j] +=1\n",
    "        else:\n",
    "            dictionary_stems[j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Despues de quitar las stopwords y los signos de puntuación hay {} palabras diferentes'.format(len(dictionary_stems)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Podemos trabajar con {} palabras menos, gracias a los lexemas'.format(len(dictionary)-len(dictionary_stems)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english['stems'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora podríamos tagger también el tipo de palabra con la libreria nltk.pos_tag\n",
    "# Vemos un ejemplo con la primera frase\n",
    "tagged = nltk.pos_tag(df_english['stems'][0])\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Al final creamos nuestro target, que consistirá en un clasificador binario partiendo de los datos que hemos conseguido,\n",
    "# con el modelo bert diferenciando los textos que tengan relación con la palabra música y los que no.\n",
    "# De este modo creamos la feature label con 1 si es musica o 0 si no lo es.\n",
    "\n",
    "df_english['Label'] = np.where(df_english.Clasif_label_zero == 'music',1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_english[['stems','Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.to_csv('../elmundo/input/df_model.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
